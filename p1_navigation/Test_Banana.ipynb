{"cells":[{"cell_type":"markdown","source":"## Test Banana Notebook\n\nThe essential steps to this are:\n\n* Define Unity environment\n* Get the default brain\n* Import DQN Agent\n* Load in checkpoint\n* Run testing loop\n\n*Note:* After you run the first cell, wait for a few seconds before running the second one.  It takes time for `unityagents` to set up.\n","metadata":{"tags":[],"cell_id":"7eb8c25a-c241-44b2-9686-66d301508bd6"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"944969a5-6bb3-495a-929a-3d24211bdcb0"},"source":"from unityagents import UnityEnvironment\nimport numpy as np\nfrom dqn_agent import Agent\nimport torch\nfrom collections import deque\nimport matplotlib.pyplot as plt","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6a80bfab-48a1-4077-a8ef-dd26bb890b2d"},"source":"env = UnityEnvironment(file_name=\"Banana_Linux_NoVis/Banana.x86_64\", worker_id=200)","execution_count":2,"outputs":[{"name":"stderr","text":"INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\t\nUnity brain name: BananaBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 37\n        Number of stacked Vector Observation: 1\n        Vector Action space type: discrete\n        Vector Action space size (per agent): 4\n        Vector Action descriptions: , , , \n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6d6b3cc0-a37a-46fc-903f-a7d15bf2eefc"},"source":"# get the default brain\nbrain_name = env.brain_names[0]\nbrain = env.brains[brain_name]","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"cf7b5459-9c45-4093-b734-fc79a38a0160"},"source":"agent = Agent(state_size=37, action_size=4, seed=0)  # Create DQN Agent\n\n# Reload the weights\nagent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"dc4441e0-83b8-459c-a9a0-a45423a13cfe"},"source":"## Loop to iterate over a single episode\nenv_info = env.reset(train_mode=False)[brain_name] # reset the environment\nstate = env_info.vector_observations[0]            # get the current state\nscore = 0                                          # initialize the score\nwhile True:\n    action = agent.act(state)                      # select an action\n    env_info = env.step(action)[brain_name]        # send the action to the environment\n    next_state = env_info.vector_observations[0]   # get the next state\n    reward = env_info.rewards[0]                   # get the reward\n    done = env_info.local_done[0]                  # see if episode has finished\n    score += reward                                # update the score\n    state = next_state                             # roll over the state to next time step\n    if done:                                       # exit loop if episode finished\n        break\n    \nprint(\"Score: {}\".format(score))","execution_count":7,"outputs":[{"name":"stdout","text":"Score: 21.0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"50815711-70fa-40b0-81b0-5c97f0a2070e"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"ec800fda-022d-4851-a7d4-022663370a8b","deepnote_execution_queue":[]}}
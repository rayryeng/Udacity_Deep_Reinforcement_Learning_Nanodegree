{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f511fc21-fcce-4fd4-9a9c-44ba16b7e868",
    "tags": []
   },
   "source": [
    "## Train Crawler Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "20ab4fa6-3543-49c5-8d2a-a5d4682c2586",
    "tags": []
   },
   "source": [
    "The essential steps to this are:\n",
    "\n",
    "* Define Unity environment\n",
    "* Get the default brain\n",
    "* Import DDPG Actor-Critic Framework\n",
    "* Run DDPG training loop with default brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "d6c89491-cb48-419c-82b4-5e3fcf3304a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from ddpg_agent import Agents\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "35c04f6c-2618-40cc-a8ae-f134a9eb7bdb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Crawler_Linux_NoVis/Crawler.x86_64\", worker_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "7250d7bc-3227-4b8d-bc4a-18e23655aedc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "68aa57f9-2613-489e-8030-a8262cc403a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.       # L2 weight decay\n",
    "UPDATE_EVERY = 20       # At every multiple of this value, we update our actor and critic\n",
    "NUM_ITERS_LEARN = 10    # When we finally do the update, we run the learning process this many times\n",
    "FC1_UNITS = 800         # Number of hidden units for the first hidden layer of the Actor and Critic networks\n",
    "FC2_UNITS = 600         # Number of hidden units for the second hidden layer of the Actor and Critic networks\n",
    "\n",
    "config = {'batch_size': BATCH_SIZE, 'buffer_size': BUFFER_SIZE, 'gamma': GAMMA,\n",
    "'tau': TAU, 'lr_actor': LR_ACTOR, 'lr_critic': LR_CRITIC, 'weight_decay': WEIGHT_DECAY,\n",
    "'update_every': UPDATE_EVERY, 'num_iters_learn': NUM_ITERS_LEARN, 'fc1_units': FC1_UNITS,\n",
    "'fc2_units': FC2_UNITS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "d29528c3-887a-466b-af01-1a8aedc6c770",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using user-defined parameters\n",
      "{'batch_size': 64,\n",
      " 'buffer_size': 1000000,\n",
      " 'fc1_units': 800,\n",
      " 'fc2_units': 600,\n",
      " 'gamma': 0.99,\n",
      " 'lr_actor': 0.0001,\n",
      " 'lr_critic': 0.001,\n",
      " 'num_iters_learn': 10,\n",
      " 'tau': 0.001,\n",
      " 'update_every': 20,\n",
      " 'weight_decay': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# size of each observation\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "# Initialise Agents\n",
    "agent = Agents(state_size=state_size, action_size=action_size, num_agents=num_agents, random_seed=1234, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "59a89447-4e6f-4461-9dad-6035097f2e06",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 12\n",
      "Action size: 20\n",
      "State size: 129\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of agents: {num_agents}')\n",
    "print(f'Action size: {action_size}')\n",
    "print(f'State size: {state_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "3a0c7030-c01e-4a75-9af3-41b66ec9a2cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Training function for DDPG\n",
    "import os\n",
    "try:\n",
    "    os.makedirs('checkpoints_crawler')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def ddpg(n_episodes=10000, max_t=-1, print_every=100, min_score=1600.0):\n",
    "    \"\"\"DDPG Learning via Actor-Critic\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode - -1 is to iterate until the episode is signalled done\n",
    "        min_score (float): the minimum score needed over the most recent 100 episodes to pass the environment\n",
    "    \"\"\"\n",
    "    scores_episode = []                # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.reset()                                          # resetting the noise process\n",
    "\n",
    "        t = 0\n",
    "        while True:\n",
    "            t += 1\n",
    "            if t == max_t:\n",
    "                break\n",
    "            actions = agent.act(states)                        # decide on the action\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "\n",
    "            next_states = env_info.vector_observations           # get next state (for each agent)\n",
    "            rewards = np.array(env_info.rewards)                 # get reward (for each agent)\n",
    "            dones = np.array(env_info.local_done, dtype=np.bool) # see if episode finished\n",
    "\n",
    "            # Perform the learning step\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            scores += np.array(env_info.rewards)               # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if any episode has finished\n",
    "                break\n",
    "\n",
    "        scores_window.append(np.mean(scores))               # save most recent averaged scores over the agents\n",
    "        scores_episode.append(np.mean(scores))              # save most recent averaged scores over the agents\n",
    "\n",
    "        print('\\rEpisode {}\\t# of timesteps needed: {}\\tAverage Score: {:.2f}'.format(i_episode, t + 1, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\t# of timesteps needed: {}\\tAverage Score: {:.2f}'.format(i_episode, t + 1, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= min_score:  # To deem this a success, we must achieve an average score of 30 in a 100 episode window\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.actor_local.state_dict(), os.path.join('checkpoints_crawler', 'checkpoint_actor.pth'))\n",
    "            torch.save(agent.critic_local.state_dict(), os.path.join('checkpoints_crawler', 'checkpoint_critic.pth'))\n",
    "            break\n",
    "    \n",
    "    # If we don't converge, save it anyway\n",
    "    if np.mean(scores_window) < min_score:\n",
    "        print(f'Note: DDPG Agent did not solve after {n_episodes} episodes')\n",
    "        torch.save(agent.actor_local.state_dict(), os.path.join('checkpoints_crawler', 'checkpoint_actor.pth'))\n",
    "        torch.save(agent.critic_local.state_dict(), os.path.join('checkpoints_crawler', 'checkpoint_critic.pth'))\n",
    "\n",
    "    return scores_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "5738367e-b646-4911-a107-54a03a904f41",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\t# of timesteps needed: 11\tAverage Score: 0.25\n",
      "Episode 200\t# of timesteps needed: 9\tAverage Score: -0.076\n",
      "Episode 300\t# of timesteps needed: 9\tAverage Score: -0.100\n",
      "Episode 400\t# of timesteps needed: 9\tAverage Score: -0.393\n",
      "Episode 500\t# of timesteps needed: 9\tAverage Score: 0.3860\n",
      "Episode 600\t# of timesteps needed: 9\tAverage Score: 0.094\n",
      "Episode 700\t# of timesteps needed: 9\tAverage Score: 0.400\n",
      "Episode 800\t# of timesteps needed: 9\tAverage Score: 0.611\n",
      "Episode 900\t# of timesteps needed: 11\tAverage Score: 0.41\n",
      "Episode 1000\t# of timesteps needed: 13\tAverage Score: 0.76\n",
      "Episode 1100\t# of timesteps needed: 8\tAverage Score: 0.946\n",
      "Episode 1200\t# of timesteps needed: 10\tAverage Score: 1.19\n",
      "Episode 1300\t# of timesteps needed: 12\tAverage Score: 1.07\n",
      "Episode 1400\t# of timesteps needed: 9\tAverage Score: 0.676\n",
      "Episode 1500\t# of timesteps needed: 9\tAverage Score: 0.379\n",
      "Episode 1600\t# of timesteps needed: 13\tAverage Score: 0.95\n",
      "Episode 1700\t# of timesteps needed: 15\tAverage Score: 1.88\n",
      "Episode 1800\t# of timesteps needed: 11\tAverage Score: 1.67\n",
      "Episode 1900\t# of timesteps needed: 13\tAverage Score: 1.23\n",
      "Episode 2000\t# of timesteps needed: 11\tAverage Score: 1.31\n",
      "Episode 2100\t# of timesteps needed: 12\tAverage Score: 1.29\n",
      "Episode 2200\t# of timesteps needed: 12\tAverage Score: 1.51\n",
      "Episode 2300\t# of timesteps needed: 15\tAverage Score: 2.05\n",
      "Episode 2400\t# of timesteps needed: 16\tAverage Score: 3.03\n",
      "Episode 2500\t# of timesteps needed: 20\tAverage Score: 3.89\n",
      "Episode 2600\t# of timesteps needed: 22\tAverage Score: 3.65\n",
      "Episode 2700\t# of timesteps needed: 25\tAverage Score: 3.61\n",
      "Episode 2800\t# of timesteps needed: 19\tAverage Score: 3.59\n",
      "Episode 2900\t# of timesteps needed: 24\tAverage Score: 4.13\n",
      "Episode 3000\t# of timesteps needed: 14\tAverage Score: 3.12\n",
      "Episode 3100\t# of timesteps needed: 23\tAverage Score: 4.07\n",
      "Episode 3200\t# of timesteps needed: 19\tAverage Score: 5.31\n",
      "Episode 3300\t# of timesteps needed: 26\tAverage Score: 6.49\n",
      "Episode 3400\t# of timesteps needed: 33\tAverage Score: 8.32\n",
      "Episode 3500\t# of timesteps needed: 30\tAverage Score: 8.47\n",
      "Episode 3600\t# of timesteps needed: 31\tAverage Score: 7.80\n",
      "Episode 3700\t# of timesteps needed: 28\tAverage Score: 6.21\n",
      "Episode 3800\t# of timesteps needed: 30\tAverage Score: 5.46\n",
      "Episode 3900\t# of timesteps needed: 29\tAverage Score: 8.00\n",
      "Episode 4000\t# of timesteps needed: 24\tAverage Score: 6.78\n",
      "Episode 4100\t# of timesteps needed: 39\tAverage Score: 4.70\n",
      "Episode 4200\t# of timesteps needed: 26\tAverage Score: 8.84\n",
      "Episode 4300\t# of timesteps needed: 25\tAverage Score: 6.63\n",
      "Episode 4400\t# of timesteps needed: 17\tAverage Score: 3.89\n",
      "Episode 4500\t# of timesteps needed: 41\tAverage Score: 6.25\n",
      "Episode 4600\t# of timesteps needed: 9\tAverage Score: 7.339\n",
      "Episode 4700\t# of timesteps needed: 26\tAverage Score: 11.51\n",
      "Episode 4800\t# of timesteps needed: 29\tAverage Score: 11.70\n",
      "Episode 4900\t# of timesteps needed: 27\tAverage Score: 10.22\n",
      "Episode 5000\t# of timesteps needed: 28\tAverage Score: 10.74\n",
      "Episode 5100\t# of timesteps needed: 9\tAverage Score: 6.8396\n",
      "Episode 5200\t# of timesteps needed: 9\tAverage Score: 6.411\n",
      "Episode 5300\t# of timesteps needed: 21\tAverage Score: 9.09\n",
      "Episode 5400\t# of timesteps needed: 27\tAverage Score: 8.053\n",
      "Episode 5500\t# of timesteps needed: 35\tAverage Score: 7.60\n",
      "Episode 5600\t# of timesteps needed: 9\tAverage Score: 6.737\n",
      "Episode 5700\t# of timesteps needed: 26\tAverage Score: 7.21\n",
      "Episode 5800\t# of timesteps needed: 23\tAverage Score: 10.03\n",
      "Episode 5900\t# of timesteps needed: 39\tAverage Score: 10.13\n",
      "Episode 6000\t# of timesteps needed: 27\tAverage Score: 10.51\n",
      "Episode 6100\t# of timesteps needed: 26\tAverage Score: 13.63\n",
      "Episode 6200\t# of timesteps needed: 27\tAverage Score: 13.19\n",
      "Episode 6300\t# of timesteps needed: 9\tAverage Score: 9.4567\n",
      "Episode 6400\t# of timesteps needed: 25\tAverage Score: 7.18\n",
      "Episode 6500\t# of timesteps needed: 27\tAverage Score: 12.67\n",
      "Episode 6600\t# of timesteps needed: 24\tAverage Score: 11.29\n",
      "Episode 6700\t# of timesteps needed: 27\tAverage Score: 12.64\n",
      "Episode 6800\t# of timesteps needed: 13\tAverage Score: 11.78\n",
      "Episode 6900\t# of timesteps needed: 9\tAverage Score: 1.1953\n",
      "Episode 7000\t# of timesteps needed: 48\tAverage Score: 3.68\n",
      "Episode 7100\t# of timesteps needed: 9\tAverage Score: 8.014\n",
      "Episode 7200\t# of timesteps needed: 15\tAverage Score: 7.26\n",
      "Episode 7300\t# of timesteps needed: 31\tAverage Score: 7.24\n",
      "Episode 7400\t# of timesteps needed: 9\tAverage Score: 4.779\n",
      "Episode 7500\t# of timesteps needed: 27\tAverage Score: 10.30\n",
      "Episode 7600\t# of timesteps needed: 29\tAverage Score: 10.38\n",
      "Episode 7700\t# of timesteps needed: 9\tAverage Score: 9.0575\n",
      "Episode 7800\t# of timesteps needed: 27\tAverage Score: 10.80\n",
      "Episode 7900\t# of timesteps needed: 29\tAverage Score: 10.44\n",
      "Episode 8000\t# of timesteps needed: 23\tAverage Score: 9.669\n",
      "Episode 8100\t# of timesteps needed: 26\tAverage Score: 10.02\n",
      "Episode 8200\t# of timesteps needed: 30\tAverage Score: 11.75\n",
      "Episode 8300\t# of timesteps needed: 27\tAverage Score: 16.16\n",
      "Episode 8400\t# of timesteps needed: 26\tAverage Score: 9.582\n",
      "Episode 8500\t# of timesteps needed: 53\tAverage Score: 13.70\n",
      "Episode 8600\t# of timesteps needed: 61\tAverage Score: 8.186\n",
      "Episode 8700\t# of timesteps needed: 14\tAverage Score: 7.33\n",
      "Episode 8800\t# of timesteps needed: 9\tAverage Score: 3.499\n",
      "Episode 8900\t# of timesteps needed: 28\tAverage Score: 3.66\n",
      "Episode 9000\t# of timesteps needed: 40\tAverage Score: 16.40\n",
      "Episode 9100\t# of timesteps needed: 30\tAverage Score: 19.59\n",
      "Episode 9200\t# of timesteps needed: 33\tAverage Score: 12.86\n",
      "Episode 9300\t# of timesteps needed: 44\tAverage Score: 13.44\n",
      "Episode 9400\t# of timesteps needed: 28\tAverage Score: 16.56\n",
      "Episode 9500\t# of timesteps needed: 38\tAverage Score: 16.18\n",
      "Episode 9600\t# of timesteps needed: 9\tAverage Score: 6.5158\n",
      "Episode 9700\t# of timesteps needed: 27\tAverage Score: 15.43\n",
      "Episode 9800\t# of timesteps needed: 34\tAverage Score: 15.27\n",
      "Episode 9900\t# of timesteps needed: 32\tAverage Score: 17.94\n",
      "Episode 10000\t# of timesteps needed: 32\tAverage Score: 18.91\n",
      "Episode 10100\t# of timesteps needed: 30\tAverage Score: 23.01\n",
      "Episode 10200\t# of timesteps needed: 37\tAverage Score: 22.71\n",
      "Episode 10300\t# of timesteps needed: 37\tAverage Score: 15.38\n",
      "Episode 10400\t# of timesteps needed: 38\tAverage Score: 16.35\n",
      "Episode 10500\t# of timesteps needed: 40\tAverage Score: 17.70\n",
      "Episode 10600\t# of timesteps needed: 46\tAverage Score: 17.13\n",
      "Episode 10700\t# of timesteps needed: 24\tAverage Score: 19.49\n",
      "Episode 10800\t# of timesteps needed: 23\tAverage Score: 13.18\n",
      "Episode 10900\t# of timesteps needed: 47\tAverage Score: 17.24\n",
      "Episode 11000\t# of timesteps needed: 33\tAverage Score: 23.01\n",
      "Episode 11100\t# of timesteps needed: 25\tAverage Score: 25.73\n",
      "Episode 11200\t# of timesteps needed: 45\tAverage Score: 25.84\n",
      "Episode 11300\t# of timesteps needed: 50\tAverage Score: 23.75\n",
      "Episode 11400\t# of timesteps needed: 27\tAverage Score: 23.53\n",
      "Episode 11500\t# of timesteps needed: 39\tAverage Score: 19.68\n",
      "Episode 11600\t# of timesteps needed: 52\tAverage Score: 20.59\n",
      "Episode 11700\t# of timesteps needed: 22\tAverage Score: 14.29\n",
      "Episode 11800\t# of timesteps needed: 55\tAverage Score: 17.78\n",
      "Episode 11900\t# of timesteps needed: 51\tAverage Score: 21.50\n",
      "Episode 12000\t# of timesteps needed: 52\tAverage Score: 21.053\n",
      "Episode 12100\t# of timesteps needed: 25\tAverage Score: 14.76\n",
      "Episode 12200\t# of timesteps needed: 42\tAverage Score: 27.786\n",
      "Episode 12300\t# of timesteps needed: 84\tAverage Score: 38.084\n",
      "Episode 12400\t# of timesteps needed: 45\tAverage Score: 31.002\n",
      "Episode 12500\t# of timesteps needed: 42\tAverage Score: 37.221\n",
      "Episode 12600\t# of timesteps needed: 56\tAverage Score: 40.424\n",
      "Episode 12700\t# of timesteps needed: 9\tAverage Score: 26.6621\n",
      "Episode 12800\t# of timesteps needed: 32\tAverage Score: 16.92\n",
      "Episode 12900\t# of timesteps needed: 48\tAverage Score: 19.19\n",
      "Episode 13000\t# of timesteps needed: 26\tAverage Score: 18.87\n",
      "Episode 13100\t# of timesteps needed: 26\tAverage Score: 20.96\n",
      "Episode 13200\t# of timesteps needed: 31\tAverage Score: 19.79\n",
      "Episode 13300\t# of timesteps needed: 26\tAverage Score: 23.04\n",
      "Episode 13400\t# of timesteps needed: 9\tAverage Score: 21.154\n",
      "Episode 13500\t# of timesteps needed: 26\tAverage Score: 21.144\n",
      "Episode 13600\t# of timesteps needed: 9\tAverage Score: 15.281\n",
      "Episode 13700\t# of timesteps needed: 48\tAverage Score: 31.210\n",
      "Episode 13800\t# of timesteps needed: 51\tAverage Score: 37.997\n",
      "Episode 13900\t# of timesteps needed: 38\tAverage Score: 35.95\n",
      "Episode 14000\t# of timesteps needed: 42\tAverage Score: 29.816\n",
      "Episode 14100\t# of timesteps needed: 35\tAverage Score: 31.273\n",
      "Episode 14200\t# of timesteps needed: 29\tAverage Score: 35.20\n",
      "Episode 14300\t# of timesteps needed: 41\tAverage Score: 36.508\n",
      "Episode 14400\t# of timesteps needed: 50\tAverage Score: 33.419\n",
      "Episode 14500\t# of timesteps needed: 37\tAverage Score: 38.653\n",
      "Episode 14600\t# of timesteps needed: 77\tAverage Score: 33.770\n",
      "Episode 14700\t# of timesteps needed: 40\tAverage Score: 36.794\n",
      "Episode 14800\t# of timesteps needed: 29\tAverage Score: 35.375\n",
      "Episode 14900\t# of timesteps needed: 73\tAverage Score: 36.08\n",
      "Episode 15000\t# of timesteps needed: 58\tAverage Score: 38.137\n",
      "Episode 15100\t# of timesteps needed: 9\tAverage Score: 9.90838\n",
      "Episode 15200\t# of timesteps needed: 41\tAverage Score: 28.707\n",
      "Episode 15300\t# of timesteps needed: 23\tAverage Score: 31.851\n",
      "Episode 15400\t# of timesteps needed: 35\tAverage Score: 27.65\n",
      "Episode 15500\t# of timesteps needed: 8\tAverage Score: 26.6472\n",
      "Episode 15600\t# of timesteps needed: 31\tAverage Score: 28.567\n",
      "Episode 15700\t# of timesteps needed: 52\tAverage Score: 36.719\n",
      "Episode 15800\t# of timesteps needed: 74\tAverage Score: 25.47\n",
      "Episode 15900\t# of timesteps needed: 10\tAverage Score: 30.786\n",
      "Episode 16000\t# of timesteps needed: 77\tAverage Score: 35.504\n",
      "Episode 16100\t# of timesteps needed: 24\tAverage Score: 29.178\n",
      "Episode 16200\t# of timesteps needed: 67\tAverage Score: 25.32\n",
      "Episode 16300\t# of timesteps needed: 42\tAverage Score: 33.124\n",
      "Episode 16400\t# of timesteps needed: 73\tAverage Score: 36.697\n",
      "Episode 16500\t# of timesteps needed: 69\tAverage Score: 39.816\n",
      "Episode 16600\t# of timesteps needed: 44\tAverage Score: 40.261\n",
      "Episode 16700\t# of timesteps needed: 22\tAverage Score: 35.789\n",
      "Episode 16800\t# of timesteps needed: 9\tAverage Score: 22.5560\n",
      "Episode 16900\t# of timesteps needed: 37\tAverage Score: 28.43\n",
      "Episode 17000\t# of timesteps needed: 37\tAverage Score: 35.121\n",
      "Episode 17100\t# of timesteps needed: 46\tAverage Score: 39.85\n",
      "Episode 17200\t# of timesteps needed: 9\tAverage Score: 22.8529\n",
      "Episode 17300\t# of timesteps needed: 31\tAverage Score: 27.994\n",
      "Episode 17400\t# of timesteps needed: 109\tAverage Score: 41.78\n",
      "Episode 17500\t# of timesteps needed: 26\tAverage Score: 42.551\n",
      "Episode 17600\t# of timesteps needed: 9\tAverage Score: 30.034\n",
      "Episode 17700\t# of timesteps needed: 9\tAverage Score: 32.654\n",
      "Episode 17800\t# of timesteps needed: 27\tAverage Score: 33.12\n",
      "Episode 17900\t# of timesteps needed: 33\tAverage Score: 39.739\n",
      "Episode 18000\t# of timesteps needed: 56\tAverage Score: 40.740\n",
      "Episode 18100\t# of timesteps needed: 35\tAverage Score: 40.078\n",
      "Episode 18200\t# of timesteps needed: 77\tAverage Score: 41.425\n",
      "Episode 18300\t# of timesteps needed: 9\tAverage Score: 32.3916\n",
      "Episode 18400\t# of timesteps needed: 37\tAverage Score: 25.75\n",
      "Episode 18500\t# of timesteps needed: 9\tAverage Score: 29.3865\n",
      "Episode 18600\t# of timesteps needed: 72\tAverage Score: 29.026\n",
      "Episode 18700\t# of timesteps needed: 63\tAverage Score: 42.55\n",
      "Episode 18800\t# of timesteps needed: 31\tAverage Score: 44.475\n",
      "Episode 18900\t# of timesteps needed: 66\tAverage Score: 42.73\n",
      "Episode 19000\t# of timesteps needed: 49\tAverage Score: 49.095\n",
      "Episode 19100\t# of timesteps needed: 50\tAverage Score: 43.842\n",
      "Episode 19200\t# of timesteps needed: 74\tAverage Score: 43.388\n",
      "Episode 19300\t# of timesteps needed: 47\tAverage Score: 34.263\n",
      "Episode 19400\t# of timesteps needed: 62\tAverage Score: 47.714\n",
      "Episode 19500\t# of timesteps needed: 25\tAverage Score: 33.526\n",
      "Episode 19600\t# of timesteps needed: 9\tAverage Score: 31.7342\n",
      "Episode 19700\t# of timesteps needed: 113\tAverage Score: 32.51\n",
      "Episode 19800\t# of timesteps needed: 94\tAverage Score: 38.900\n",
      "Episode 19900\t# of timesteps needed: 87\tAverage Score: 50.983\n",
      "Episode 20000\t# of timesteps needed: 28\tAverage Score: 52.006\n",
      "Episode 20100\t# of timesteps needed: 75\tAverage Score: 38.035\n",
      "Episode 20200\t# of timesteps needed: 57\tAverage Score: 27.683\n",
      "Episode 20300\t# of timesteps needed: 9\tAverage Score: 34.0013\n",
      "Episode 20400\t# of timesteps needed: 9\tAverage Score: 21.2119\n",
      "Episode 20500\t# of timesteps needed: 91\tAverage Score: 15.334\n",
      "Episode 20600\t# of timesteps needed: 80\tAverage Score: 29.901\n",
      "Episode 20700\t# of timesteps needed: 28\tAverage Score: 55.406\n",
      "Episode 20800\t# of timesteps needed: 44\tAverage Score: 49.538\n",
      "Episode 20900\t# of timesteps needed: 102\tAverage Score: 46.62\n",
      "Episode 21000\t# of timesteps needed: 57\tAverage Score: 42.198\n",
      "Episode 21100\t# of timesteps needed: 9\tAverage Score: 35.3452\n",
      "Episode 21200\t# of timesteps needed: 28\tAverage Score: 31.837\n",
      "Episode 21300\t# of timesteps needed: 75\tAverage Score: 45.253\n",
      "Episode 21400\t# of timesteps needed: 23\tAverage Score: 27.630\n",
      "Episode 21500\t# of timesteps needed: 58\tAverage Score: 20.607\n",
      "Episode 21600\t# of timesteps needed: 58\tAverage Score: 33.966\n",
      "Episode 21700\t# of timesteps needed: 35\tAverage Score: 41.830\n",
      "Episode 21800\t# of timesteps needed: 53\tAverage Score: 49.714\n",
      "Episode 21900\t# of timesteps needed: 41\tAverage Score: 49.507\n",
      "Episode 22000\t# of timesteps needed: 140\tAverage Score: 40.13\n",
      "Episode 22100\t# of timesteps needed: 41\tAverage Score: 59.863\n",
      "Episode 22200\t# of timesteps needed: 54\tAverage Score: 56.750\n",
      "Episode 22300\t# of timesteps needed: 32\tAverage Score: 44.099\n",
      "Episode 22400\t# of timesteps needed: 25\tAverage Score: 45.248\n",
      "Episode 22500\t# of timesteps needed: 51\tAverage Score: 48.611\n",
      "Episode 22600\t# of timesteps needed: 48\tAverage Score: 67.039\n",
      "Episode 22700\t# of timesteps needed: 87\tAverage Score: 55.064\n",
      "Episode 22800\t# of timesteps needed: 115\tAverage Score: 52.62\n",
      "Episode 22900\t# of timesteps needed: 9\tAverage Score: 51.4315\n",
      "Episode 23000\t# of timesteps needed: 9\tAverage Score: 20.6222\n",
      "Episode 23100\t# of timesteps needed: 36\tAverage Score: 41.027\n",
      "Episode 23200\t# of timesteps needed: 49\tAverage Score: 48.438\n",
      "Episode 23300\t# of timesteps needed: 65\tAverage Score: 54.291\n",
      "Episode 23400\t# of timesteps needed: 72\tAverage Score: 51.488\n",
      "Episode 23500\t# of timesteps needed: 9\tAverage Score: 37.5687\n",
      "Episode 23600\t# of timesteps needed: 43\tAverage Score: 38.121\n",
      "Episode 23700\t# of timesteps needed: 68\tAverage Score: 58.837\n",
      "Episode 23800\t# of timesteps needed: 101\tAverage Score: 57.72\n",
      "Episode 23900\t# of timesteps needed: 24\tAverage Score: 51.516\n",
      "Episode 24000\t# of timesteps needed: 111\tAverage Score: 68.87\n",
      "Episode 24100\t# of timesteps needed: 49\tAverage Score: 60.032\n",
      "Episode 24200\t# of timesteps needed: 42\tAverage Score: 59.160\n",
      "Episode 24300\t# of timesteps needed: 26\tAverage Score: 57.169\n",
      "Episode 24400\t# of timesteps needed: 32\tAverage Score: 51.749\n",
      "Episode 24500\t# of timesteps needed: 82\tAverage Score: 58.464\n",
      "Episode 24600\t# of timesteps needed: 40\tAverage Score: 53.784\n",
      "Episode 24700\t# of timesteps needed: 41\tAverage Score: 63.069\n",
      "Episode 24800\t# of timesteps needed: 131\tAverage Score: 84.64\n",
      "Episode 24900\t# of timesteps needed: 60\tAverage Score: 77.305\n",
      "Episode 25000\t# of timesteps needed: 27\tAverage Score: 62.343\n",
      "Episode 25100\t# of timesteps needed: 98\tAverage Score: 71.578\n",
      "Episode 25200\t# of timesteps needed: 61\tAverage Score: 56.410\n",
      "Episode 25300\t# of timesteps needed: 38\tAverage Score: 61.747\n",
      "Episode 25400\t# of timesteps needed: 72\tAverage Score: 59.033\n",
      "Episode 25500\t# of timesteps needed: 46\tAverage Score: 56.694\n",
      "Episode 25600\t# of timesteps needed: 43\tAverage Score: 57.276\n",
      "Episode 25700\t# of timesteps needed: 70\tAverage Score: 57.858\n",
      "Episode 25800\t# of timesteps needed: 62\tAverage Score: 56.093\n",
      "Episode 25900\t# of timesteps needed: 70\tAverage Score: 56.405\n",
      "Episode 26000\t# of timesteps needed: 44\tAverage Score: 50.559\n",
      "Episode 26100\t# of timesteps needed: 51\tAverage Score: 56.184\n",
      "Episode 26200\t# of timesteps needed: 9\tAverage Score: 46.3217\n",
      "Episode 26300\t# of timesteps needed: 113\tAverage Score: 59.97\n",
      "Episode 26400\t# of timesteps needed: 23\tAverage Score: 54.672\n",
      "Episode 26500\t# of timesteps needed: 63\tAverage Score: 60.755\n",
      "Episode 26600\t# of timesteps needed: 71\tAverage Score: 57.418\n",
      "Episode 26700\t# of timesteps needed: 45\tAverage Score: 63.436\n",
      "Episode 26800\t# of timesteps needed: 48\tAverage Score: 61.574\n",
      "Episode 26900\t# of timesteps needed: 46\tAverage Score: 65.722\n",
      "Episode 27000\t# of timesteps needed: 47\tAverage Score: 72.034\n",
      "Episode 27100\t# of timesteps needed: 65\tAverage Score: 63.542\n",
      "Episode 27200\t# of timesteps needed: 46\tAverage Score: 56.830\n",
      "Episode 27300\t# of timesteps needed: 40\tAverage Score: 48.064\n",
      "Episode 27400\t# of timesteps needed: 47\tAverage Score: 61.196\n",
      "Episode 27500\t# of timesteps needed: 33\tAverage Score: 61.893\n",
      "Episode 27600\t# of timesteps needed: 66\tAverage Score: 30.521\n",
      "Episode 27700\t# of timesteps needed: 48\tAverage Score: 51.626\n",
      "Episode 27800\t# of timesteps needed: 75\tAverage Score: 57.260\n",
      "Episode 27900\t# of timesteps needed: 60\tAverage Score: 64.575\n",
      "Episode 28000\t# of timesteps needed: 55\tAverage Score: 61.790\n",
      "Episode 28100\t# of timesteps needed: 26\tAverage Score: 62.187\n",
      "Episode 28200\t# of timesteps needed: 43\tAverage Score: 73.804\n",
      "Episode 28300\t# of timesteps needed: 26\tAverage Score: 56.609\n",
      "Episode 28400\t# of timesteps needed: 9\tAverage Score: 46.1721\n",
      "Episode 28500\t# of timesteps needed: 1002\tAverage Score: nan0\n",
      "Episode 28527\t# of timesteps needed: 1002\tAverage Score: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7465a4599d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b50529a81b3f>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(n_episodes, max_t, print_every, min_score)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m                        \u001b[0;31m# decide on the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# all actions between -1 and 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;31m# send all actions to the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m           \u001b[0;31m# get next state (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.7/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.7/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36m_generate_step_input\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    487\u001b[0m                     \u001b[0mtext_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 )\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0mrl_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0mrl_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_unity_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = ddpg(n_episodes=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c51675e1-640f-4aaf-9346-5adb9aac4d25",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Average Score over 12 Agents')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "112732d4-1cf3-4d03-aea7-164beb2e970f",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
